{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BLIP NoCaps Evaluation (Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Bật Internet trong notebook Kaggle, thêm dataset chứa `nocap_val_4500_captions.json` và `nocaps_test_image_info.json` vào tab *Data*.\n",
        "- Notebook sẽ tự tải ảnh thiếu dựa trên `coco_url`; nếu có lỗi kết nối, xem log và xử lý thủ công.\n",
        "- GPU Kaggle (P100/V100) chạy ổn với batch size ≤ 4; nếu OOM hãy giảm thêm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "BASE_DIR = Path(\"/kaggle/working\")\n",
        "DATASET_ROOT = Path(\"/kaggle/input/nocaps\")\n",
        "VAL_JSON = DATASET_ROOT / \"nocap_val_4500_captions.json\"\n",
        "TEST_JSON = DATASET_ROOT / \"nocaps_test_image_info.json\"\n",
        "REPO_URL = \"https://github.com/TMG301-Skubidu/Image-Captioning-Vietnamese-BLIP.git\"\n",
        "REPO_DIR = BASE_DIR / \"BLIP\"\n",
        "IMAGE_ROOT = BASE_DIR / \"nocaps_images\"\n",
        "ANN_ROOT = BASE_DIR / \"nocaps_annotations\"\n",
        "OUTPUT_DIR = BASE_DIR / \"NoCaps\"\n",
        "\n",
        "for path in [IMAGE_ROOT / \"val\", IMAGE_ROOT / \"test\", ANN_ROOT, OUTPUT_DIR]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"VAL_JSON exists: {VAL_JSON.exists()} -> {VAL_JSON}\")\n",
        "print(f\"TEST_JSON exists: {TEST_JSON.exists()} -> {TEST_JSON}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
        "else:\n",
        "    print(f\"Repository already available at {REPO_DIR}\")\n",
        "\n",
        "commit = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=REPO_DIR).decode().strip()\n",
        "print(\"Repository commit:\", commit)\n",
        "for rel_path in [\"eval_nocaps.py\", \"data/nocaps_dataset.py\", \"configs/nocaps.yaml\"]:\n",
        "    path = REPO_DIR / rel_path\n",
        "    status = \"OK\" if path.exists() else \"MISSING\"\n",
        "    print(f\"{rel_path}: {status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "requirements_path = REPO_DIR / \"requirements.txt\"\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(requirements_path)], check=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import timm\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(\"Working directory:\", Path.cwd())\n",
        "print(\"Torch:\", torch.__version__, \"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Timm:\", timm.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_image(entry):\n",
        "    keys = [\"file_name\", \"id\", \"domain\", \"coco_url\", \"open_images_id\"]\n",
        "    return {k: entry.get(k) for k in keys if k in entry}\n",
        "\n",
        "with open(VAL_JSON, \"r\") as f:\n",
        "    val_raw = json.load(f)\n",
        "with open(TEST_JSON, \"r\") as f:\n",
        "    test_raw = json.load(f)\n",
        "\n",
        "val_caps = val_raw.get(\"annotations\", [])\n",
        "print(\"Val keys:\", list(val_raw.keys()))\n",
        "print(\"Val images:\", len(val_raw[\"images\"]))\n",
        "print(\"Val annotations:\", len(val_caps))\n",
        "print(\"Val sample image:\", summarize_image(val_raw[\"images\"][0]))\n",
        "print(\"Val sample captions:\", [ann[\"caption\"] for ann in val_caps[:3]])\n",
        "\n",
        "print(\"Test keys:\", list(test_raw.keys()))\n",
        "print(\"Test images:\", len(test_raw[\"images\"]))\n",
        "print(\"Test has annotations:\", \"annotations\" in test_raw)\n",
        "print(\"Test sample image:\", summarize_image(test_raw[\"images\"][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "def fetch_image(url, dest, retries=3, timeout=20):\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    last_err = None\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            response = session.get(url, timeout=timeout)\n",
        "            if response.status_code == 200:\n",
        "                dest.write_bytes(response.content)\n",
        "                return True, None\n",
        "            last_err = f\"HTTP {response.status_code}\"\n",
        "        except Exception as exc:\n",
        "            last_err = str(exc)\n",
        "        time.sleep(min(5, attempt))\n",
        "    return False, last_err\n",
        "\n",
        "def convert_split(raw_data, split):\n",
        "    images = raw_data[\"images\"]\n",
        "    annotations = raw_data.get(\"annotations\", [])\n",
        "    id2captions = {}\n",
        "    for ann in annotations:\n",
        "        image_id = int(ann[\"image_id\"])\n",
        "        id2captions.setdefault(image_id, []).append(ann[\"caption\"])\n",
        "    converted = []\n",
        "    failures = []\n",
        "    for img in tqdm(images, desc=f\"{split} images\", unit=\"img\"):\n",
        "        file_name = img[\"file_name\"]\n",
        "        url = img.get(\"coco_url\") or img.get(\"flickr_url\")\n",
        "        if not url:\n",
        "            failures.append({\"file_name\": file_name, \"reason\": \"missing_url\"})\n",
        "            continue\n",
        "        dest = IMAGE_ROOT / split / file_name\n",
        "        if not dest.exists():\n",
        "            success, err = fetch_image(url, dest)\n",
        "            if not success:\n",
        "                failures.append({\"file_name\": file_name, \"reason\": err})\n",
        "                continue\n",
        "        entry = {\"image\": f\"{split}/{file_name}\", \"img_id\": int(img[\"id\"])}\n",
        "        if id2captions:\n",
        "            entry[\"captions\"] = id2captions.get(int(img[\"id\"]), [])\n",
        "        converted.append(entry)\n",
        "    return converted, failures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_entries, val_failures = convert_split(val_raw, \"val\")\n",
        "test_entries, test_failures = convert_split(test_raw, \"test\")\n",
        "\n",
        "val_out = ANN_ROOT / \"nocaps_val.json\"\n",
        "test_out = ANN_ROOT / \"nocaps_test.json\"\n",
        "\n",
        "with open(val_out, \"w\") as f:\n",
        "    json.dump(val_entries, f)\n",
        "with open(test_out, \"w\") as f:\n",
        "    json.dump(test_entries, f)\n",
        "\n",
        "failures_log = OUTPUT_DIR / \"download_failures.json\"\n",
        "with open(failures_log, \"w\") as f:\n",
        "    json.dump({\"val\": val_failures, \"test\": test_failures}, f, indent=2)\n",
        "\n",
        "print(f\"Val entries saved: {len(val_entries)} -> {val_out}\")\n",
        "print(f\"Val download failures: {len(val_failures)}\")\n",
        "if val_failures[:3]:\n",
        "    print(\"Val failure samples:\", val_failures[:3])\n",
        "\n",
        "print(f\"Test entries saved: {len(test_entries)} -> {test_out}\")\n",
        "print(f\"Test download failures: {len(test_failures)}\")\n",
        "if test_failures[:3]:\n",
        "    print(\"Test failure samples:\", test_failures[:3])\n",
        "\n",
        "if val_failures or test_failures:\n",
        "    print(f\"WARNING: some images failed to download. See {failures_log} for details.\")\n",
        "else:\n",
        "    print(\"All images downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ruamel_yaml as yaml\n",
        "\n",
        "orig_config_path = Path(\"configs/nocaps.yaml\")\n",
        "with open(orig_config_path, \"r\") as f:\n",
        "    config = yaml.load(f, Loader=yaml.Loader)\n",
        "\n",
        "config[\"image_root\"] = str(IMAGE_ROOT).replace(\"\\\\\", \"/\")\n",
        "config[\"ann_root\"] = str(ANN_ROOT).replace(\"\\\\\", \"/\")\n",
        "config[\"batch_size\"] = min(4, config.get(\"batch_size\", 32))\n",
        "\n",
        "config_path = Path(\"/kaggle/working/nocaps_eval.yaml\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "print(\"Config written to:\", config_path)\n",
        "print(json.dumps(config, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python eval_nocaps.py --config /kaggle/working/nocaps_eval.yaml --output_dir /kaggle/working/NoCaps --device cuda --distributed False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_dir = OUTPUT_DIR / \"result\"\n",
        "val_result_path = result_dir / \"val.json\"\n",
        "test_result_path = result_dir / \"test.json\"\n",
        "\n",
        "with open(val_result_path, \"r\") as f:\n",
        "    val_result = json.load(f)\n",
        "with open(test_result_path, \"r\") as f:\n",
        "    test_result = json.load(f)\n",
        "\n",
        "print(\"Val captions:\", len(val_result))\n",
        "print(\"Test captions:\", len(test_result))\n",
        "print(\"Sample val captions:\")\n",
        "for sample in val_result[:5]:\n",
        "    print(sample)\n",
        "\n",
        "print(\"Artifacts saved under:\", OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "- Nếu `download_failures.json` không trống, cân nhắc tải thủ công các ảnh đó rồi chạy lại ô chuyển đổi.\n",
        "- Nén `nocaps_images`, `nocaps_annotations`, `NoCaps/result/test.json` thành dataset Kaggle riêng để notebook lần sau chạy nhanh hơn.\n",
        "- Upload `NoCaps/result/test.json` lên server NoCaps để nhận điểm chính thức."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
